{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiattn/mgmt476-analytics-portfolio/blob/main/MGMT467_PromptPlusExamples_Colab_Kaggle_GCS_BQ_DQ_NH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFGDrMH6oASh"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "GFGDrMH6oASh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZFI8W0joASi"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "FZFI8W0joASi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwpFpFQYoASj"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "vwpFpFQYoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYFZvZKhoASj"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "QYFZvZKhoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd75y1hAoASj"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "Fd75y1hAoASj"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Prompt the user to upload their kaggle.json file.\n",
        "# This file contains your Kaggle API credentials.\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create the .kaggle directory if it doesn't exist.\n",
        "# This is where Kaggle expects to find the credentials file.\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Save the uploaded file to the correct location.\n",
        "# Using the first uploaded file as we expect only one (kaggle.json).\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "\n",
        "# Set file permissions to 0600 (owner read/write only).\n",
        "# This is crucial for security to prevent other users from accessing your API key.\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "# Verify the Kaggle installation by printing the version.\n",
        "# This confirms the CLI is installed and can access the credentials.\n",
        "!kaggle --version\n",
        "\n",
        "# Done: Kaggle setup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "KjXQxZVJZ-Bc",
        "outputId": "1c3037bd-cd5d-4989-f91f-7b15a604b58d"
      },
      "id": "KjXQxZVJZ-Bc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af8f4fdb-da82-4dba-a982-35bf70c32330\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af8f4fdb-da82-4dba-a982-35bf70c32330\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Vrae8faFmI"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "Z8Vrae8faFmI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHHGMZsHoASj",
        "outputId": "034e6055-0828-4dde-fc0d-f13a9845ad29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt-467-nh\n",
            "Project: mgmt-467-nh | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt-467-nh\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE (from LLM) — Auth + Project/Region (commented; write your own cell using the prompt)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "xHHGMZsHoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEIykdfioASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "BEIykdfioASk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: Check active project and region\n",
        "!gcloud config get-value project\n",
        "import os\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKQVtt4cSqUn",
        "outputId": "13f26e6d-b00f-4e12-f2c6-de705b629dda"
      },
      "id": "gKQVtt4cSqUn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mgmt-467-nh\n",
            "Project: mgmt-467-nh | Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOb_eekKoASk"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?\n",
        "\n",
        "Setting the PROJECT_ID and REGION at the beginning of the notebook is crucial for several reasons:\n",
        "\n",
        "    Consistency: It ensures all subsequent commands and resource creations (like BigQuery datasets and GCS buckets) happen within the intended project and geographical region. This prevents accidental creation of resources in unintended locations.\n",
        "\n",
        "    Cost Management: Resources in different regions can have varying costs. Explicitly setting the region helps manage costs by keeping resources in a desired location.\n",
        "    Latency: Choosing a region closer to where the data is being processed or accessed can reduce latency.\n",
        "    \n",
        "    Reproducibility: By setting these values programmatically, the notebook becomes more reproducible. Someone else running the notebook will create resources in the same specified project and region.\n"
      ],
      "id": "TOb_eekKoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gf8c7gpoASk"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "3Gf8c7gpoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjRY9WSFoASk"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "MjRY9WSFoASk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "DEnJj-7doASk",
        "outputId": "c39bd437-1273-4458-927d-72cfc40732e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6af9a197-47a1-4ce1-98f2-44bf6bd3431b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6af9a197-47a1-4ce1-98f2-44bf6bd3431b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "\n",
        "!kaggle --version"
      ],
      "id": "DEnJj-7doASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thMOKW2ZoASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "thMOKW2ZoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB2Vy-teoASk"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?\n",
        "\n",
        "Requiring strict 0600 permissions on API tokens like kaggle.json is a critical security measure. The 0600 permission setting means that only the owner of the file (in this case, the user running the Colab notebook) can read and write to the file. No other user or process on the system can access it.\n",
        "\n",
        "API tokens grant programmatic access to your accounts and data on platforms like Kaggle. If other users or processes could read your kaggle.json file, they could potentially steal your credentials and access your data or perform actions on your behalf."
      ],
      "id": "pB2Vy-teoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9kJSE4oASl"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "BD9kJSE4oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRhSvGi4oASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "vRhSvGi4oASl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory to store raw data\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using Kaggle CLI to /content/data\n",
        "# The -d flag specifies the dataset, and -p specifies the download path\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "# -o flag overwrites files if they exist\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes in a neat table\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8_U7PESet1j",
        "outputId": "a2f4e70f-f23f-4b5f-a229-5ebc73bb1b5f"
      },
      "id": "L8_U7PESet1j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 808MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFvFr29hoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "hFvFr29hoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "they2Q5foASl"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?\n",
        "\n",
        "Keeping a clean file inventory is useful downstream for several reasons. It provides a clear record of the raw data used, essential for reproducibility and auditing the data pipeline. This inventory also helps in troubleshooting by quickly identifying if expected files are present or if sizes are consistent. Ultimately, it serves as valuable documentation and facilitates automation of subsequent data processing steps."
      ],
      "id": "they2Q5foASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5PQ-P1yoASl"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "X5PQ-P1yoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IZooOUoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "b_IZooOUoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3xtQmu5oASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c49f89-df36-42b1-cf32-04a1d90301d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating bucket: gs://mgmt467-netflix-nh in region: us-central1\n",
            "Creating gs://mgmt467-netflix-nh/...\n",
            "\n",
            "Uploading files to gs://mgmt467-netflix-nh/netflix/\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-nh/netflix/movies.csv\n",
            "Copying file:///content/data/raw/README.md to gs://mgmt467-netflix-nh/netflix/README.md\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-nh/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-nh/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-nh/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-nh/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-nh/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 8.5MiB/s\n",
            "\n",
            "Successfully created bucket: mgmt467-netflix-nh and uploaded files.\n",
            "\n",
            "Benefits of staging data in GCS:\n",
            "- **Consistent Source:** GCS provides a stable and versionable location for your data.\n",
            "- **Scalability:** GCS is highly scalable, handling large datasets easily.\n",
            "- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\n",
            "- **Cost-Effective:** GCS can be a cost-effective storage solution.\n",
            "\n",
            "Verifying contents of gs://mgmt467-netflix-nh/netflix/:\n",
            "gs://mgmt467-netflix-nh/netflix/README.md\n",
            "gs://mgmt467-netflix-nh/netflix/movies.csv\n",
            "gs://mgmt467-netflix-nh/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-nh/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-nh/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-nh/netflix/users.csv\n",
            "gs://mgmt467-netflix-nh/netflix/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "# Create a unique bucket in the specified region\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "bucket_name = f\"mgmt467-netflix-nh\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "print(f\"Creating bucket: gs://{bucket_name} in region: {os.environ.get('REGION')}\")\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSVs to the bucket\n",
        "print(f\"\\nUploading files to gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name and explain staging benefits\n",
        "print(f\"\\nSuccessfully created bucket: {bucket_name} and uploaded files.\")\n",
        "print(\"\\nBenefits of staging data in GCS:\")\n",
        "print(\"- **Consistent Source:** GCS provides a stable and versionable location for your data.\")\n",
        "print(\"- **Scalability:** GCS is highly scalable, handling large datasets easily.\")\n",
        "print(\"- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\")\n",
        "print(\"- **Cost-Effective:** GCS can be a cost-effective storage solution.\")\n",
        "\n",
        "# Verify contents (optional but recommended)\n",
        "print(f\"\\nVerifying contents of gs://{bucket_name}/netflix/:\")\n",
        "!gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "O3xtQmu5oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZhif_knoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "kZhif_knoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwgInaDFoASl"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab.\n",
        "\n",
        "Staging in GCS offers several benefits over loading directly from Colab. Firstly, GCS provides persistent and scalable storage, meaning your data is safe and accessible even after your Colab session ends, unlike temporary Colab storage. Secondly, GCS integrates seamlessly with BigQuery and other Google Cloud services, making subsequent data processing and analysis much more efficient and cost-effective. Finally, GCS supports versioning, providing a built-in audit trail and the ability to easily revert to previous data states if needed."
      ],
      "id": "CwgInaDFoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ApPnyBoASl"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "k7ApPnyBoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mvCSfFhoASl"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "3mvCSfFhoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKjSOx8JoASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69447b32-dd51-486d-83c6-a2e59ac532c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'mgmt-467-nh:netflix' successfully created.\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "RKjSOx8JoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bDr5GH1oASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5bc011-1b42-4c17-c253-a535b33e2e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-nh/netflix/users.csv\n",
            "Waiting on bqjob_r5df111377ac340e3_00000199ee8b633f_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-nh/netflix/movies.csv\n",
            "Waiting on bqjob_rf20ed433e19e38b_00000199ee8b83f3_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-nh/netflix/watch_history.csv\n",
            "Waiting on bqjob_r696cef08ff257e6d_00000199ee8ba5a2_1 ... (3s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-nh/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r7be52592f6e01794_00000199ee8bcfe0_1 ... (2s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-nh/netflix/search_logs.csv\n",
            "Waiting on bqjob_r387faedd008e4784_00000199ee8bf508_1 ... (3s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-nh/netflix/reviews.csv\n",
            "Waiting on bqjob_r4c509afa41dc19b0_00000199ee8c209f_1 ... (1s) Current status: DONE   \n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| users      | 20600 |\n",
            "+------------+-------+\n",
            "+------------+------+\n",
            "| table_name |  n   |\n",
            "+------------+------+\n",
            "| movies     | 2080 |\n",
            "+------------+------+\n",
            "+---------------+--------+\n",
            "|  table_name   |   n    |\n",
            "+---------------+--------+\n",
            "| watch_history | 210000 |\n",
            "+---------------+--------+\n",
            "+---------------------+--------+\n",
            "|     table_name      |   n    |\n",
            "+---------------------+--------+\n",
            "| recommendation_logs | 104000 |\n",
            "+---------------------+--------+\n",
            "+-------------+-------+\n",
            "| table_name  |   n   |\n",
            "+-------------+-------+\n",
            "| search_logs | 53000 |\n",
            "+-------------+-------+\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| reviews    | 30900 |\n",
            "+------------+-------+\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Load tables (commented)\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "DATASET=\"netflix\" # Define DATASET variable\n",
        "\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  # Corrected bq load command structure\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts\n",
        "for tbl in tables.keys():\n",
        "  # Using f-strings for correct formatting of the SQL query and enclosing in single quotes\n",
        "  !bq query --nouse_legacy_sql 'SELECT \"{tbl}\" AS table_name, COUNT(*) AS n FROM `{os.environ[\"GOOGLE_CLOUD_PROJECT\"]}.{DATASET}.{tbl}`'"
      ],
      "id": "3bDr5GH1oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqEU0_sBoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "yqEU0_sBoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeM4Ik7moASl"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?\n",
        "\n",
        "Autodetect is acceptable for initial data exploration or when dealing with frequently changing schemas, as it's quick and easy. However, you should enforce explicit schemas in production pipelines or when data types are critical for analysis and consistency. Explicit schemas prevent unexpected type conversions or errors due to schema drift, ensuring data integrity and reliable queries."
      ],
      "id": "xeM4Ik7moASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TOxNJk_oASl"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "3TOxNJk_oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nht9dQqOoASl"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "nht9dQqOoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D2oFpiSoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "4D2oFpiSoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08HEz5PooASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806bdc97-0015-4f6d-d3bc-f0220dc1e65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((20600, 0.0, 0.0, 11.93), {'n': 0, 'pct_missing_country': 1, 'pct_missing_subscription_plan': 2, 'pct_missing_age': 3})\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Users: % missing per column\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "08HEz5PooASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si9Z_OIEoASm"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "Si9Z_OIEoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6zI2MSmoASm"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why.\n",
        "\n",
        "Based on the previous query output, the age column has the most missing values (11.93%). If age is more likely to be missing for certain demographics or subscription tiers, it could be MAR (Missing At Random)."
      ],
      "id": "n6zI2MSmoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T73XrgnIoASm"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "T73XrgnIoASm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZegFSkoASm"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "I5ZegFSkoASm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-xK-vcoASm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260fa09c-1ed5-4ca7-9b82-df31546382e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('user_03310', 'movie_0640', datetime.date(2024, 9, 8), 'Smart TV', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00391', 'movie_0893', datetime.date(2024, 8, 26), 'Laptop', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03408', 'movie_0146', datetime.date(2025, 6, 2), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06085', 'movie_0346', datetime.date(2024, 3, 4), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03898', 'movie_0500', datetime.date(2025, 7, 29), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00472', 'movie_0719', datetime.date(2024, 12, 4), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03140', 'movie_0205', datetime.date(2025, 9, 11), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08276', 'movie_0460', datetime.date(2024, 4, 24), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02652', 'movie_0352', datetime.date(2024, 10, 22), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08826', 'movie_0133', datetime.date(2025, 4, 11), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04899', 'movie_0142', datetime.date(2025, 1, 20), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_05952', 'movie_0893', datetime.date(2024, 4, 29), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_07617', 'movie_0785', datetime.date(2024, 7, 14), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02822', 'movie_0009', datetime.date(2025, 8, 30), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06535', 'movie_0890', datetime.date(2025, 5, 23), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_09512', 'movie_0825', datetime.date(2025, 1, 7), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02976', 'movie_0987', datetime.date(2024, 9, 19), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_07738', 'movie_0793', datetime.date(2025, 7, 28), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01143', 'movie_0166', datetime.date(2024, 5, 28), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03348', 'movie_0688', datetime.date(2024, 1, 22), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "AH-xK-vcoASm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GsXzPdQoASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca51d88-f092-44fb-885b-0e3973f38508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deduplicated table watch_history_dedup created successfully.\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `{project_id}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"Deduplicated table watch_history_dedup created successfully.\")"
      ],
      "id": "_GsXzPdQoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mrF9O0UoASq"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "6mrF9O0UoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyNoGyV0oASq"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?\n",
        "\n",
        "Duplicates can arise from natural user behavior (like refreshing a page) or be system-generated due to logging errors or retries. They corrupt labels and KPIs by artificially inflating counts, leading to inaccurate analysis and potentially flawed decisions. For example, duplicate watch events would overstate engagement metrics."
      ],
      "id": "vyNoGyV0oASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFlt_N03oASq"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "YFlt_N03oASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGiM4dQToASq"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "QGiM4dQToASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5zQ5TJmoASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52971a5-5a05-40cc-a829-a1d8bb726ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((3489, 100000, 3.49), {'outliers': 0, 'total': 1, 'pct_outliers': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — IQR outlier rate (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "T5zQ5TJmoASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuY6Qv5UoASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba9eb84-c6de-4e32-aac6-b4558f116495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('before', [0.2, 24.8, 41.8, 61.3, 91.7, 799.3]), {'which': 0, 'q': 1})\n",
            "Row(('after', [4.4, 24.6, 41.5, 61.5, 92.0, 203.6]), {'which': 0, 'q': 1})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(98)] AS p99\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h, q;\n",
        "\n",
        "-- Quantiles before vs after\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "OuY6Qv5UoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYXOx0WioASq"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "iYXOx0WioASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gubOakeIoASq"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why.\n",
        "\n",
        "Capping can be harmful if outliers represent genuine, important data points that are critical for certain analyses or model predictions. Capping might distort the true distribution and relationships in the data. Tree-based models like Random Forests or Gradient Boosting Machines are generally less sensitive to outliers because they make decisions based on thresholds rather than the magnitude of individual data points."
      ],
      "id": "gubOakeIoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtXNlTftoASq"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "xtXNlTftoASq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOJ7CAHkoASq"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "VOJ7CAHkoASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfvn5Vn7oASq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b30ecf-5102-4da2-a527-eab931494708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0, 100000, 0.0), {'sessions_over_8h': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes_capped > 8*60)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "Qfvn5Vn7oASq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CglM1uhBoASr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1780b64c-f3b4-4704-d357-dca1d6708bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((358, 20600, 1.74), {'extreme_age_rows': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "CglM1uhBoASr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kho40L1oASr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72863c5e-73c4-4a63-e207-b57f7a39e9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((24, 22, 2080, 2.21), {'titles_under_15m': 0, 'titles_over_8h': 1, 'total': 2, 'pct_duration_anomaly': 3})\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 480) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "2kho40L1oASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yhdFbkaoASr"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "5yhdFbkaoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZK1fkr4oASr"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?\n",
        "\n",
        "Based on the output of the previous cells, the flag_age_extreme anomaly flag is the most common at 1.74%. I would keep this as a feature because extreme ages could indicate data entry errors or represent a niche user segment that behaves differently, which could be valuable for targeted recommendations or other business insights. The other flags (binge sessions and duration anomalies) had 0.0% and 2.21% respectively, making age extreme the most frequent."
      ],
      "id": "pZK1fkr4oASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKDRvHtoASr"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "8yKDRvHtoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy3qqUtyoASr"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "Iy3qqUtyoASr"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
